{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Scoring Models\n\nThe following notebook will demonstrate how to use Earth-2 MIP to perform a scoring\nworkflow to assess the accuracy of AI models using ERA5 reanalysis data as the\nground truth. This can then be extended to score custom models placed into the model\nregistry. This tutorial also covers details about using a HDF5 datasource, the expected\nformat this data should be formatted and how to use H5 files for evaluating models over\na year's worth of data.\n\nIn summary this notebook will cover the following topics:\n\n- Implementing a basic scoring workflow in Earth-2 MIP\n- HDF5 datasource and the expected data format of the H5 files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import datetime\nimport os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "\"\"\"\n## Setting up HDF5 data\n\nThe first step of scoring is handling the target data. One could simply use the\nCDSDatasource to download target data on the fly, but depending on how comprehensive\nthe scoring is, this can prove quite slow.\nAdditionally, many scoring pipelines require on-prem data.\nThus, this will demonstrate how to use the HDF5 datasource.\n\nThe HDF5 data source assumes that the data to be loaded is stored in the general form:\n\nyear.h5\n | - field (time, channels, grid)\n\nFor AFNO which requires 34 channels with a time-step size of 6 hours, an H5 file will\nhave the following form of data:\n\n2017.h5\n | - field (1460, 34, 720, 1440)\n2016.h5\n | - field (1464, 34, 720, 1440)\n2015.h5\n | - field (1460, 34, 720, 1440)\n\n(Note the later two dims have some flexibility with regridding)\n\nOne option to build these H5 files from scratch is to use the ERA5 mirror scripts\nprovided in [Modulus](https://github.com/NVIDIA/modulus/tree/main/examples/weather/dataset_download).\nFor the rest of this tutorial, it is assumed that 2017.h5 is present for the full year.\n\"\"\"  # noqa: E501"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "load an values in a .env file where you can specify your H5 root folder by\nadding a line like:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    import dotenv\n\n    dotenv.load_dotenv()\nexcept ModuleNotFoundError:\n    pass  # pip install python-dotenv\n\n# can set this with the export ERA5_HDF5=/path/to/root/of/h5/files\nh5_folder = os.getenv(\"ERA5_HDF5\", \"/mount/73vars\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "\"\"\"\n## Loading Models\n\nWith the HDF5 datasource loaded the next step is to load our model we wish to score.\nIn this tutorial we will be using the built in FourcastNet model.\nTake note of the `e2mip://` which will direct Earth-2 MIP to load a known model package.\nFourcastNet is selected here simply because its a 34 channel model which aligns with the\nH5 files described above.\n\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from modulus.distributed import DistributedManager\n\nfrom earth2mip import registry\nfrom earth2mip.networks import dlwp\n\ndevice = DistributedManager().device\npackage = registry.get_model(\"e2mip://dlwp\")\nmodel = dlwp.load(package, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "\"\"\"\n## HDF5 Datasource\n\nWith the H5 files properly formatted, a H5 datasource can now get defined.\nThis requires two items: a root directory location of the H5 files as well as some\nmetadata. Se\nThe metadata is a JSON/dictionary object that helps Earth-2 MIP index the H5 file.\nTypically, this can be done by placing a `data.json` file next to the H5 files.\nSee [this documentation](https://github.com/NVIDIA/earth2mip/blob/f44c580ccc3d98bf349fe97823bb1540e532c80d/earth2mip/initial_conditions/hdf5.py#L38)\nfor more details on how to set up input data correctly.\n\"\"\"  # noqa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from earth2mip.initial_conditions import hdf5\n\ndatasource = hdf5.DataSource.from_path(\n    root=h5_folder, channel_names=model.channel_names\n)\n\n# Test to see if our datasource is working\ntime = datetime.datetime(2017, 5, 1, 18)\nout = datasource[time]\nprint(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "\"\"\"\n## Running Scoring\n\nWith the datasource and model loaded, scoring can now be performed.\nTo score this we will run 10 day forecasts over the span of the entire year at 30 day\nintervals.\nFor research, one would typically want this to be much more comprehensive so feel free\nto customize for you're use case.\n\nThe `score_deterministic` API provides a simple way to calculate RMSE and ACC scores.\nACC scores require climatology which is beyond the scope of this example, thus zero\nvalues will be provided and only the RMSE will be of concern.\n\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from earth2mip.inference_medium_range import save_scores, time_average_metrics\n\ntime = datetime.datetime(2017, 1, 1, 0)\ninitial_times = [time + datetime.timedelta(days=30 * i) for i in range(12)]\n\n# Output directoy\noutput_dir = \"outputs/03_model_scoring\"\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir, exist_ok=True)\n    output = save_scores(\n        model,\n        n=20,  # 12 hour timesteps\n        initial_times=initial_times,\n        data_source=datasource,\n        time_mean=datasource.time_means,\n        output_directory=output_dir,\n    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "\"\"\"\n## Post Processing\n\nThe last step is any post processing / IO that is desired.\nTypically its recommended to save the output dataset to a netCDF file for further\nprocessing.\nLets plot the RMSE of the z500 (geopotential at pressure level 500) field.\n\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom earth2mip.forecast_metrics_io import read_metrics\n\nseries = read_metrics(output_dir)\ndataset = time_average_metrics(series)\n\nplt.close(\"all\")\nfig, ax = plt.subplots(figsize=(10, 5))\nt = dataset.lead_time / pd.Timedelta(\"1 h\")\ny = dataset.rmse.sel(channel=\"z500\")\nax.plot(t, y)\nax.set_xlabel(\"Lead Time (hours)\")\nax.set_ylabel(\"RMSE\")\nax.set_title(\"DLWP z500 RMSE 2017\")\nplt.savefig(f\"{output_dir}/dwlp_z500_rmse.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "\"\"\"\nThis completes this introductory notebook on basic scoring of models in Earth-2 MIP,\nwhich is founational for comparing the performance of different models.\n\"\"\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}